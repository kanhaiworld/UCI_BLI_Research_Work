{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq diffusers datasets accelerate wandb open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.cuda.device_count())  # Should return number of GPUs detected\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)  # Should return a version number (e.g., '11.8')\n",
    "print(torch.backends.cudnn.enabled)  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from diffusers import DDIMScheduler, DDPMPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device) # should be cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "image_pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = image_pipe().images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every step the model is provided a noisy image and is asked to predict the noise (and thus an estimate of what the fully denoised image looks like)\n",
    "# Scheduler handles the sampling. Start w/ random NxN image and then for every step in schedulers time step we feed noisy input of size NxN to the model and pass prediction back the the step() function\n",
    "\n",
    "scheduler = DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "scheduler.set_timesteps(num_inference_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random starting point\n",
    "x = torch.randn(4, 3, 256, 256).to(device)  # Batch of 4, 3-channel 256 x 256 px images\n",
    "\n",
    "# Loop through the sampling timesteps\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "\n",
    "    # Prepare model input\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "    # Get the prediction\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "\n",
    "    # Calculate what the updated sample should look like with the scheduler\n",
    "    scheduler_output = scheduler.step(noise_pred, t, x)\n",
    "\n",
    "    # Update x\n",
    "    x = scheduler_output.prev_sample\n",
    "\n",
    "    # Occasionally display both x and the predicted denoised images\n",
    "    if i % 10 == 0 or i == len(scheduler.timesteps) - 1:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        grid = torchvision.utils.make_grid(x, nrow=4).permute(1, 2, 0)\n",
    "        axs[0].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
    "        axs[0].set_title(f\"Current x (step {i})\")\n",
    "\n",
    "        pred_x0 = scheduler_output.pred_original_sample  # Not available for all schedulers\n",
    "        grid = torchvision.utils.make_grid(pred_x0, nrow=4).permute(1, 2, 0)\n",
    "        axs[1].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
    "        axs[1].set_title(f\"Predicted denoised images (step {i})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HUGGING FACE DATASET\n",
    "\n",
    "##### \n",
    "# FINE TUNING: \n",
    "# @markdown load and prepare a dataset:\n",
    "# Not on Colab? Comments with #@ enable UI tweaks like headings or user inputs\n",
    "# but can safely be ignored if you're working on a different platform.\n",
    "\n",
    "dataset_name = \"huggan/smithsonian_butterflies_subset\"  # @param\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "image_size = 256  # @param\n",
    "batch_size = 4  # @param\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Previewing batch:\")\n",
    "batch = next(iter(train_dataloader))\n",
    "grid = torchvision.utils.make_grid(batch[\"images\"], nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE TUNE WITH FACE DATASET PATH - G:\\Kanhai\\HCII_Dataset\\Data_Collection\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "image_folder = r\"G:\\Kanhai\\HCII_Dataset\\Data_Collection\"  # Use raw string (r\"...\") for Windows paths\n",
    "image_size = 256\n",
    "batch_size = 4\n",
    "\n",
    "# Define preprocessing transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # No class label, just return the image tensor\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomImageDataset(image_folder, transform=preprocess)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Preview batch of images\n",
    "print(\"Previewing batch:\")\n",
    "batch = next(iter(train_dataloader))  # Only images, no labels\n",
    "grid = torchvision.utils.make_grid(batch, nrow=4)\n",
    "\n",
    "# Show images\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(0, 1))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for the training loop. Weâ€™ll update the weights of the pre-trained model by setting the optimization target to image_pipe.unet.parameters()\n",
    "\n",
    "num_epochs = 2  # @param\n",
    "lr = 1e-5  # 2param\n",
    "grad_accumulation_steps = 2  # @param\n",
    "\n",
    "optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            image_pipe.scheduler.num_train_timesteps,\n",
    "            (bs,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction for the noise\n",
    "        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # Compare the prediction with the actual noise:\n",
    "        loss = F.mse_loss(\n",
    "            noise_pred, noise\n",
    "        )  # NB - trying to predict noise (eps) not (noisy_ims-clean_ims) or just (clean_ims)\n",
    "\n",
    "        # Store for later plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update the model parameters with the optimizer based on this loss\n",
    "        loss.backward(loss)\n",
    "\n",
    "        # Gradient accumulation:\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch} average loss: {sum(losses[-len(train_dataloader):])/len(train_dataloader)}\")\n",
    "\n",
    "# Plot the loss curve:\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 2\n",
    "lr = 1e-5\n",
    "grad_accumulation_steps = 2\n",
    "\n",
    "optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        clean_images = batch.to(device)  # âœ… Fix applied here\n",
    "\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            image_pipe.scheduler.num_train_timesteps,\n",
    "            (bs,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction for the noise\n",
    "        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # Compare the prediction with the actual noise:\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Store for later plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update the model parameters with the optimizer based on this loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation:\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} average loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Generate and plot some images:\n",
    "x = torch.randn(8, 3, 256, 256).to(device)  # Batch of 8\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
